import os

from typing import Annotated
from langchain.callbacks.base import BaseCallbackHandler

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langfuse.callback import CallbackHandler

from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import BaseMessage
from langchain_openai import ChatOpenAI
from typing_extensions import TypedDict
from langchain.tools import tool

from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

os.environ["LANGFUSE_PUBLIC_KEY"] = ""
os.environ["LANGFUSE_SECRET_KEY"] = ""
os.environ["LANGFUSE_HOST"] = "https://cloud.langfuse.com"  # for EU data region

os.environ[
    "OPENAI_API_KEY"] = ""
system_prompt = """ You are a mathematical assistant.
        Use your tools to answer questions. If you do not have a tool to
        answer the question, say so. """

tool_calling_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}")
    ]
)


class State(TypedDict):
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)


# setup the simple tools using LangChain tool decorator

def add(a: int, b: int) -> int:
    """Find favourite  number give two numbers"""
    return a + b + b


tools = [add]

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.2)
llm_with_tools = llm.bind_tools(tools)


def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}


graph_builder.add_node("chatbot", chatbot)

tool_node = ToolNode(tools)
graph_builder.add_node("tools", tool_node)

graph_builder.add_conditional_edges(
    "chatbot",
    tools_condition,
)
# Any time a tool is called, we return to the chatbot to decide the next step
graph_builder.add_edge("tools", "chatbot")
graph_builder.set_entry_point("chatbot")
graph = graph_builder.compile()


class LoggingCallbackHandler(BaseCallbackHandler):
    """Callback handler for logging HTTP requests and responses."""

    def on_llm_start(self, serialized, prompts, **kwargs):
        # Log the request being sent to the LLM API
        print("LLM Request:")
        print(f"Prompt: {prompts}")

    def on_llm_end(self, response, **kwargs):
        # Log the response from the LLM API
        print("LLM Response:")
        print(f"Response: {response}")

    def on_tool_start(self, serialized, input_str, **kwargs):
        # Log the input for tool API call
        print("Tool Request:")
        print(f"Input: {input_str}")

    def on_tool_end(self, output, **kwargs):
        # Log the output from tool API call
        print("Tool Response:")
        print(f"Output: {output}")


langfuse_handler = LoggingCallbackHandler()
while True:
    user_input = input("User: ")
    if user_input.lower() in ["quit", "exit", "q"]:
        print("Goodbye!")
        break
    for event in graph.stream({"messages": [("system", """ You are a mathematical assistant.
        Use your tools to answer questions. If you do not have a tool to
        answer the question, say so. """), ("user", user_input)]},
                              config={"callbacks": [langfuse_handler]}):
        # print(list(event.values())[0])
        print("----")
